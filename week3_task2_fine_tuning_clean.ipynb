{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"week3_task2_fine_tuning_clean.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"RakzENpw7-1T","colab_type":"text"},"cell_type":"markdown","source":["# Fine-tuning InceptionV3 for flowers classification\n","\n","In this task you will fine-tune InceptionV3 architecture for flowers classification task.\n","\n","InceptionV3 architecture (https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html):\n","<img src=\"images/inceptionv3.png\" style=\"width:70%\">\n","\n","Flowers classification dataset (http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) consists of 102 flower categories commonly occurring in the United Kingdom. Each class contains between 40 and 258 images:\n","<img src=\"images/flowers.jpg\" style=\"width:70%\">"]},{"metadata":{"id":"dBPCT1_k7-1Y","colab_type":"text"},"cell_type":"markdown","source":["# Import stuff"]},{"metadata":{"scrolled":false,"id":"YXP3vLNJ7-1j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"outputId":"f64bde14-4d32-4e2e-c8de-8b73f5b17563","executionInfo":{"status":"error","timestamp":1546170444673,"user_tz":0,"elapsed":961,"user":{"displayName":"Chinemelu Ezeh","photoUrl":"https://lh3.googleusercontent.com/-_RQJf3fVsB0/AAAAAAAAAAI/AAAAAAAAAHA/HNmeSi0ez14/s64/photo.jpg","userId":"16450813448508143253"}}},"cell_type":"code","source":["import sys\n","sys.path.append(\"..\")\n","import grading\n","import download_utils"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-77232fc4619a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgrading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'grading'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"metadata":{"id":"hvSA2eao7-1w","colab_type":"code","colab":{}},"cell_type":"code","source":["# !!! remember to clear session/graph if you rebuild your graph to avoid out-of-memory errors !!!"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"bEzoNxZH7-15","colab_type":"code","colab":{}},"cell_type":"code","source":["# download_utils.link_all_keras_resources()"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:39.210959Z","start_time":"2017-09-03T13:00:39.057800Z"},"scrolled":true,"id":"v1VGZatT7-1_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":402},"outputId":"7550fd89-e404-47fa-c3bb-1c3c55e8ff35","executionInfo":{"status":"error","timestamp":1546170447704,"user_tz":0,"elapsed":3970,"user":{"displayName":"Chinemelu Ezeh","photoUrl":"https://lh3.googleusercontent.com/-_RQJf3fVsB0/AAAAAAAAAAI/AAAAAAAAAHA/HNmeSi0ez14/s64/photo.jpg","userId":"16450813448508143253"}}},"cell_type":"code","source":["import tensorflow as tf\n","import keras\n","from keras import backend as K\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","print(tf.__version__)\n","print(keras.__version__)\n","import cv2  # for image processing\n","from sklearn.model_selection import train_test_split\n","import scipy.io\n","import os\n","import tarfile\n","import keras_utils\n","from keras_utils import reset_tf_session "],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["1.12.0\n","2.2.4\n"],"name":"stdout"},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-9d57d986ada8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreset_tf_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"metadata":{"id":"2UBTznWh7-2O","colab_type":"text"},"cell_type":"markdown","source":["# Fill in your Coursera token and email\n","To successfully submit your answers to our grader, please fill in your Coursera submission token and email"]},{"metadata":{"scrolled":false,"id":"KvNpPXKy7-2R","colab_type":"code","colab":{}},"cell_type":"code","source":["grader = grading.Grader(assignment_key=\"2v-uxpD7EeeMxQ6FWsz5LA\", \n","                        all_parts=[\"wuwwC\", \"a4FK1\", \"qRsZ1\"])"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"RNF9_kF37-2X","colab_type":"code","colab":{}},"cell_type":"code","source":["# token expires every 30 min\n","COURSERA_TOKEN = '7RU8Pp2J0VhjbWJk'\n","COURSERA_EMAIL = 'chinemeluezeh@gmail.com'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NiUoPghT7-2b","colab_type":"text"},"cell_type":"markdown","source":["# Load dataset"]},{"metadata":{"id":"ACcrP8ox7-2c","colab_type":"text"},"cell_type":"markdown","source":["Dataset was downloaded for you, it takes 12 min and 400mb.\n","Relevant links (just in case):\n","- http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html\n","- http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n","- http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat"]},{"metadata":{"scrolled":false,"id":"gYqZWLzO7-2e","colab_type":"code","colab":{}},"cell_type":"code","source":["# we downloaded them for you, just link them here\n","# download_utils.link_week_3_resources()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rER3v97i7-2j","colab_type":"text"},"cell_type":"markdown","source":["# Prepare images for model"]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:39.214524Z","start_time":"2017-09-03T13:00:39.212453Z"},"scrolled":false,"id":"fOeCcims7-2l","colab_type":"code","colab":{}},"cell_type":"code","source":["# we will crop and resize input images to IMG_SIZE x IMG_SIZE\n","IMG_SIZE = 250"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-10T12:46:09.790818Z","start_time":"2017-09-10T12:46:09.777771Z"},"scrolled":false,"id":"S5_hoFOl7-2q","colab_type":"code","colab":{}},"cell_type":"code","source":["def decode_image_from_raw_bytes(raw_bytes):\n","    img = cv2.imdecode(np.asarray(bytearray(raw_bytes), dtype=np.uint8), 1)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    return img"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pl1EwShj7-2t","colab_type":"text"},"cell_type":"markdown","source":["We will take a center crop from each image like this:\n","<img src=\"images/center_crop.jpg\" style=\"width:50%\">"]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:39.393675Z","start_time":"2017-09-03T13:00:39.302130Z"},"scrolled":false,"id":"a6EvU_yw7-2u","colab_type":"code","colab":{}},"cell_type":"code","source":["def image_center_crop(img):\n","    \"\"\"\n","    Makes a square center crop of an img, which is a [h, w, 3] numpy array.\n","    Returns [min(h, w), min(h, w), 3] output with same width and height.\n","    For cropping use numpy slicing.\n","    \"\"\"\n","    dim = img.shape\n","    size = min(dim[0], dim[1])\n","    xs = int((dim[0]-size)/2)\n","    ys = int((dim[1] - size)/2)\n","    \n","    cropped_img = img[xs:xs+size, ys:ys+size] ### YOUR CODE HERE\n","    \n","    return cropped_img"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:39.506473Z","start_time":"2017-09-03T13:00:39.395470Z"},"scrolled":false,"id":"oVBGQBiZ7-2y","colab_type":"code","colab":{}},"cell_type":"code","source":["def prepare_raw_bytes_for_model(raw_bytes, normalize_for_model=True):\n","    img = decode_image_from_raw_bytes(raw_bytes)  # decode image raw bytes to matrix\n","    img = image_center_crop(img)  # take squared center crop\n","    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # resize for our model\n","    if normalize_for_model:\n","        img = img.astype(\"float32\")  # prepare for normalization\n","        img = keras.applications.inception_v3.preprocess_input(img)  # normalize for model\n","    return img"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"3jO7gQSU7-21","colab_type":"code","colab":{}},"cell_type":"code","source":["# reads bytes directly from tar by filename (slow, but ok for testing, takes ~6 sec)\n","def read_raw_from_tar(tar_fn, fn):\n","    with tarfile.open(tar_fn) as f:\n","        m = f.getmember(fn)\n","        return f.extractfile(m).read()"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:39.961301Z","start_time":"2017-09-03T13:00:39.508004Z"},"scrolled":false,"id":"vdigm1fw7-24","colab_type":"code","colab":{}},"cell_type":"code","source":["# test cropping\n","raw_bytes = read_raw_from_tar(\"102flowers.tgz\", \"jpg/image_00001.jpg\")\n","\n","img = decode_image_from_raw_bytes(raw_bytes)\n","print(img.shape)\n","plt.imshow(img)\n","plt.show()\n","\n","img = prepare_raw_bytes_for_model(raw_bytes, normalize_for_model=False)\n","print(img.shape)\n","plt.imshow(img)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"sr-mImxJ7-29","colab_type":"code","colab":{}},"cell_type":"code","source":["## GRADED PART, DO NOT CHANGE!\n","# Test image preparation for model\n","prepared_img = prepare_raw_bytes_for_model(read_raw_from_tar(\"102flowers.tgz\", \"jpg/image_00001.jpg\"))\n","grader.set_answer(\"qRsZ1\", list(prepared_img.shape) + [np.mean(prepared_img), np.std(prepared_img)])"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"B9m97Crz7-3A","colab_type":"code","colab":{}},"cell_type":"code","source":["# you can make submission with answers so far to check yourself at this stage\n","# grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PLwfFhko7-3D","colab_type":"text"},"cell_type":"markdown","source":["# Prepare for training"]},{"metadata":{"scrolled":false,"id":"ff169RI77-3E","colab_type":"code","colab":{}},"cell_type":"code","source":["# read all filenames and labels for them\n","\n","# read filenames firectly from tar\n","def get_all_filenames(tar_fn):\n","    with tarfile.open(tar_fn) as f:\n","        return [m.name for m in f.getmembers() if m.isfile()]\n","\n","all_files = sorted(get_all_filenames(\"102flowers.tgz\"))  # list all files in tar sorted by name\n","all_labels = scipy.io.loadmat('imagelabels.mat')['labels'][0] - 1  # read class labels (0, 1, 2, ...)\n","# all_files and all_labels are aligned now\n","N_CLASSES = len(np.unique(all_labels))\n","print(N_CLASSES)"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:40.185940Z","start_time":"2017-09-03T13:00:40.175758Z"},"scrolled":false,"id":"79_K6sJq7-3J","colab_type":"code","colab":{}},"cell_type":"code","source":["# split into train/test\n","tr_files, te_files, tr_labels, te_labels = \\\n","    train_test_split(all_files, all_labels, test_size=0.2, random_state=42, stratify=all_labels)"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"ZnWH-rqg7-3L","colab_type":"code","colab":{}},"cell_type":"code","source":["# will yield raw image bytes from tar with corresponding label\n","def raw_generator_with_label_from_tar(tar_fn, files, labels):\n","    label_by_fn = dict(zip(files, labels))\n","    with tarfile.open(tar_fn) as f:\n","        while True:\n","            m = f.next()\n","            if m is None:\n","                break\n","            if m.name in label_by_fn:\n","                yield f.extractfile(m).read(), label_by_fn[m.name]"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:40.529088Z","start_time":"2017-09-03T13:00:40.423114Z"},"scrolled":false,"id":"1_n6Ioyu7-3O","colab_type":"code","colab":{}},"cell_type":"code","source":["# batch generator\n","BATCH_SIZE = 32\n","\n","def batch_generator(items, batch_size):\n","    \"\"\"\n","    Implement batch generator that yields items in batches of size batch_size.\n","    There's no need to shuffle input items, just chop them into batches.\n","    Remember about the last batch that can be smaller than batch_size!\n","    Input: any iterable (list, generator, ...). You should do `for item in items: ...`\n","        In case of generator you can pass through your items only once!\n","    Output: In output yield each batch as a list of items.\n","    \"\"\"\n","    count = 0\n","    my_list = []\n","    each_list =[]\n","    for item in items:\n","        each_list.append(item)\n","        count = count + 1\n","        if (count == batch_size):\n","            my_list.append(each_list)\n","            each_list =[]\n","            count = 0\n","    if len(each_list) != 0:\n","        my_list.append(each_list)\n","\n","        \n","            \n","    return my_list\n","    \n","    ### YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"rjJ-zPOF7-3Q","colab_type":"code","colab":{}},"cell_type":"code","source":["## GRADED PART, DO NOT CHANGE!\n","# Test batch generator\n","def _test_items_generator():\n","    for i in range(10):\n","        yield i\n","\n","grader.set_answer(\"a4FK1\", list(map(lambda x: len(x), batch_generator(_test_items_generator(), 3))))\n","print(batch_generator(_test_items_generator(), 3))"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"31iil9AC7-3S","colab_type":"code","colab":{}},"cell_type":"code","source":["# you can make submission with answers so far to check yourself at this stage\n","# grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:40.637615Z","start_time":"2017-09-03T13:00:40.530642Z"},"scrolled":false,"id":"L5Yu11TX7-3T","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_generator(files, labels):\n","    while True:  # so that Keras can loop through this as long as it wants\n","        for batch in batch_generator(raw_generator_with_label_from_tar(\n","                \"102flowers.tgz\", files, labels), BATCH_SIZE):\n","            # prepare batch images\n","            batch_imgs = []\n","            batch_targets = []\n","            for raw, label in batch:\n","                img = prepare_raw_bytes_for_model(raw)\n","                batch_imgs.append(img)\n","                batch_targets.append(label)\n","            # stack images into 4D tensor [batch_size, img_size, img_size, 3]\n","            batch_imgs = np.stack(batch_imgs, axis=0)\n","            # convert targets into 2D tensor [batch_size, num_classes]\n","            batch_targets = keras.utils.np_utils.to_categorical(batch_targets, N_CLASSES)\n","            yield batch_imgs, batch_targets"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:41.092659Z","start_time":"2017-09-03T13:00:40.639132Z"},"scrolled":false,"id":"VgWNdJ3Z7-3V","colab_type":"code","colab":{}},"cell_type":"code","source":["# test training generator\n","for _ in train_generator(tr_files, tr_labels):\n","    print(_[0].shape, _[1].shape)\n","    plt.imshow(np.clip(_[0][0] / 2. + 0.5, 0, 1))\n","    break"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-vOw7gxL7-3X","colab_type":"text"},"cell_type":"markdown","source":["# Training"]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-10T13:16:35.109044Z","start_time":"2017-09-10T13:16:35.105127Z"},"id":"GgLPJiYP7-3Y","colab_type":"text"},"cell_type":"markdown","source":["You cannot train such a huge architecture from scratch with such a small dataset.\n","\n","But using fine-tuning of last layers of pre-trained network you can get a pretty good classifier very quickly."]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:41.097588Z","start_time":"2017-09-03T13:00:41.094216Z"},"scrolled":false,"id":"yaKka1BV7-3Y","colab_type":"code","colab":{}},"cell_type":"code","source":["# remember to clear session if you start building graph from scratch!\n","s = reset_tf_session()\n","# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:41.222209Z","start_time":"2017-09-03T13:00:41.098974Z"},"scrolled":false,"id":"j3-nnyXh7-3a","colab_type":"code","colab":{}},"cell_type":"code","source":["def inception(use_imagenet=True):\n","    # load pre-trained model graph, don't add final layer\n","    model = keras.applications.InceptionV3(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3),\n","                                          weights='imagenet' if use_imagenet else None)\n","    # add global pooling just like in InceptionV3\n","    new_output = keras.layers.GlobalAveragePooling2D()(model.output)\n","    # add new dense layer for our labels\n","    new_output = keras.layers.Dense(N_CLASSES, activation='softmax')(new_output)\n","    model = keras.engine.training.Model(model.inputs, new_output)\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:45.150429Z","start_time":"2017-09-03T13:00:41.223777Z"},"scrolled":false,"id":"jJQimKRl7-3c","colab_type":"code","colab":{}},"cell_type":"code","source":["model = inception()"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:45.252883Z","start_time":"2017-09-03T13:00:45.152062Z"},"scrolled":false,"id":"MBLtdTUO7-3e","colab_type":"code","colab":{}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:45.273005Z","start_time":"2017-09-03T13:00:45.254250Z"},"scrolled":false,"id":"LsdklcI87-3g","colab_type":"code","colab":{}},"cell_type":"code","source":["# how many layers our model has\n","print(len(model.layers))"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:45.370171Z","start_time":"2017-09-03T13:00:45.274354Z"},"scrolled":false,"id":"UEbjImxD7-3j","colab_type":"code","colab":{}},"cell_type":"code","source":["# set all layers trainable by default\n","for layer in model.layers:\n","    layer.trainable = True\n","    if isinstance(layer, keras.layers.BatchNormalization):\n","        # we do aggressive exponential smoothing of batch norm\n","        # parameters to faster adjust to our new dataset\n","        layer.momentum = 0.9\n","    \n","# fix deep layers (fine-tuning only last 50)\n","for layer in model.layers[:-50]:\n","    # fix all but batch norm layers, because we neeed to update moving averages for a new dataset!\n","    if not isinstance(layer, keras.layers.BatchNormalization):\n","        layer.trainable = False"],"execution_count":0,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T13:00:45.494833Z","start_time":"2017-09-03T13:00:45.371512Z"},"scrolled":false,"id":"sjOHrUBI7-3l","colab_type":"code","colab":{}},"cell_type":"code","source":["# compile new model\n","model.compile(\n","    loss='categorical_crossentropy',  # we train 102-way classification\n","    optimizer=keras.optimizers.adamax(lr=1e-2),  # we can take big lr here because we fixed first layers\n","    metrics=['accuracy']  # report accuracy during training\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cnTEFO4d7-3n","colab_type":"code","colab":{}},"cell_type":"code","source":["# we will save model checkpoints to continue training in case of kernel death\n","model_filename = 'flowers.{0:03d}.hdf5'\n","last_finished_epoch = None\n","\n","#### uncomment below to continue training from model checkpoint\n","#### fill `last_finished_epoch` with your latest finished epoch\n","from keras.models import load_model\n","s = reset_tf_session()\n","last_finished_epoch = 1\n","model = load_model(model_filename.format(last_finished_epoch))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fw7w-NSs7-3p","colab_type":"code","colab":{}},"cell_type":"code","source":["print(model)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-LkGQVuy7-3s","colab_type":"text"},"cell_type":"markdown","source":["Training takes **2 hours**. You're aiming for ~0.93 validation accuracy."]},{"metadata":{"ExecuteTime":{"end_time":"2017-09-03T14:23:36.792701Z","start_time":"2017-09-03T13:00:45.496194Z"},"scrolled":true,"id":"xxxfh0Xw7-3t","colab_type":"code","colab":{}},"cell_type":"code","source":["# fine tune for 2 epochs (full passes through all training data)\n","# we make 2*8 epochs, where epoch is 1/8 of our training data to see progress more often\n","model.fit_generator(\n","    train_generator(tr_files, tr_labels), \n","    steps_per_epoch=len(tr_files) // BATCH_SIZE // 8,\n","    epochs=2 * 8,\n","    validation_data=train_generator(te_files, te_labels), \n","    validation_steps=len(te_files) // BATCH_SIZE // 4,\n","    callbacks=[keras_utils.TqdmProgressCallback(), \n","               keras_utils.ModelSaveCallback(model_filename)],\n","    verbose=0,\n","    initial_epoch=last_finished_epoch or 0\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"3d4gph8r7-3w","colab_type":"code","colab":{}},"cell_type":"code","source":["## GRADED PART, DO NOT CHANGE!\n","# Accuracy on validation set\n","test_accuracy = model.evaluate_generator(\n","    train_generator(te_files, te_labels), \n","    len(te_files) // BATCH_SIZE // 2\n",")[1]\n","grader.set_answer(\"wuwwC\", test_accuracy)\n","print(test_accuracy)"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"FZvB8vSM7-3y","colab_type":"code","colab":{}},"cell_type":"code","source":["# you can make submission with answers so far to check yourself at this stage\n","grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iNrwTgXH7-3z","colab_type":"text"},"cell_type":"markdown","source":["That's it! Congratulations!\n","\n","What you've done:\n","- prepared images for the model\n","- implemented your own batch generator\n","- fine-tuned the pre-trained model"]},{"metadata":{"id":"fDS9YK-07-30","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}